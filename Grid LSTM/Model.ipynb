{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  GRID LSTM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is Meant by the GRID LSTM**\n",
    "The GLSTM-A model is a proposed deep learning model designed for efficient short-term traffic flow prediction, addressing limitations in existing models like LSTM, TCN, and Conv-LSTM by focusing on temporal prediction without the need for spatial data. \n",
    "\n",
    "Key points of the model include:\n",
    "\n",
    "**Parallel LSTM Grid with Attention:**\n",
    "\n",
    "- The core architecture consists of a parallel LSTM grid network to extract periodicity information from long-term historical data, improving accuracy over traditional models.\n",
    "- The attention mechanism enhances the modelâ€™s focus on important features, further boosting prediction accuracy.\n",
    "Efficient Temporal Prediction:\n",
    "\n",
    "- Unlike hybrid models (e.g., Conv-LSTM, TCN) that use spatial data, the GLSTM-A is tailored for scenarios where spatial information is not available.\n",
    "It captures recurring patterns within specific time frames through LSTM layers, proficiently modeling temporal relationships in both long- and short-term data.\n",
    "\n",
    "**Lower Memory Requirement:**\n",
    "\n",
    "- The architecture is simpler than advanced models like Conv-LSTM and TCN, leading to reduced memory requirements, making it suitable for resource-constrained devices (e.g., Raspberry Pi).\n",
    "\n",
    "GLSTM-A demonstrates its versatility in various traffic scenarios, making it suitable for intelligent transportation systems and resource-limited environments like edge devices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Below are the Required Modules for this Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required Models for the project\n",
    "\n",
    "import pandas as pd # type: ignore\n",
    "import numpy as  np # type: ignore\n",
    "import os # type: ignore\n",
    "import math # type: ignore\n",
    "import matplotlib.pyplot as plt # type: ignore\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler # type: ignore\n",
    "from tensorflow.keras.models import Model , load_model, save_model # type: ignore\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Concatenate, Conv1D, TimeDistributed, Add, Multiply # type: ignore\n",
    "from tensorflow.keras.losses import MeanSquaredError # type: ignore\n",
    "from tensorflow.keras.optimizers import Adam # type: ignore\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have to load our dataset required for the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'PATH_TO_YOUR_DATASET.csv' # Path to your dataset\n",
    "dataset = pd.read_csv(file_path) # Load the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make sure There is no zero values in the dataset,we have to check is there any null values in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_values = dataset.isnull().sum() # Check for null values in the dataset\n",
    "print(\"Null values in each column:\\n\", null_values) # Print the null values in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalizing the Required Data in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, We have to get the required data values in between 0 and 1 to make the caculations simpler.\n",
    "\n",
    "So, we are using the Normalizatioin Method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column = ['Column Name'] # Column name to be normalized\n",
    "scaler = MinMaxScaler() # Normalizing the data\n",
    "dataset[column] = scaler.fit_transform(dataset[column]) # Normalizing the data\n",
    "dataset.head() # Displaying the first 5 rows of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to divide the data into the sequences according to the required number of TimeIntervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lstm_sequences(data, time_steps, start_idx):\n",
    "    X, Y = [], []\n",
    "    for i in range(len(data) - time_steps):\n",
    "        X.append(data[i:i + time_steps])\n",
    "        Y.append(data[i + time_steps])\n",
    "    return np.array(X), np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_steps = \"YOUR_TIME_STEPS\" # Time steps for the LSTM model\n",
    "start_idx = 8064 # Start index for the dataset to take the 3 months data for the grid lstm\n",
    "flow_data = dataset['Column Name'].values[start_idx:]\n",
    "\n",
    "lstm_sequences = flow_data.reshape(-1, 1) # Reshape the data for the LSTM model\n",
    "lstm_X, y = create_lstm_sequences(lstm_sequences, time_steps, start_idx) # Create LSTM sequences\n",
    "\n",
    "# Print the shape of the LSTM input and output\n",
    "\n",
    "print(lstm_X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, We have to reshape the data to the required shape so we are reshaping the data to the day wise format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_and_combine(df, files=13, rows_per_file=2016, rows_target=91):\n",
    "    reshaped_data = []\n",
    "    for i in range(files):\n",
    "        start_idx = i * rows_per_file\n",
    "        end_idx = start_idx + rows_per_file\n",
    "        num_rows = end_idx - start_idx\n",
    "        target_shape = (num_rows // 7, 7)\n",
    "        reshaped_segment = df[start_idx:end_idx].reshape(target_shape)\n",
    "        reshaped_data.append(reshaped_segment)\n",
    "    final_data = np.concatenate(reshaped_data, axis=1)\n",
    "    return pd.DataFrame(final_data)\n",
    "\n",
    "reshaped_dataset = reshape_and_combine(dataset['Flow (Veh/5 Minutes)'].values)\n",
    "reshaped_dataset = np.array(reshaped_dataset)\n",
    "\n",
    "# Transpose the array to get the shape (91, 288)\n",
    "reshaped_station = reshaped_dataset.T\n",
    "\n",
    "print(reshaped_station.shape)  # Should output (91, 288)\n",
    "print(reshaped_station)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "j = 0\n",
    "time_steps=4\n",
    "k=time_steps-1\n",
    "sequence_length=15\n",
    "sequences=[]\n",
    "sequence=[]\n",
    "\n",
    "while i < reshaped_station.shape[0]-time_steps+1 and j <reshaped_station.shape[1]-sequence_length+1:\n",
    "  sequence.append(reshaped_station[i][0+j:sequence_length+j])\n",
    "  if i >= k:\n",
    "    i=k-time_steps\n",
    "    j += 1\n",
    "    sequences.append(sequence)\n",
    "    sequence=[]\n",
    "\n",
    "  if  j >=  reshaped_station.shape[1]-sequence_length+1:\n",
    "    k+=1\n",
    "    j=0\n",
    "    i=k-time_steps\n",
    "\n",
    "  i += 1\n",
    "\n",
    "print(np.array(sequences).shape)\n",
    "print(np.array(sequences[0]).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spliting the Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now , we have to split the data for training and testing the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_sequences = min(len(lstm_X), len(sequences))\n",
    "\n",
    "lstm_sequences = lstm_sequences[:min_sequences]\n",
    "sequences = sequences[:min_sequences]\n",
    "sequences = np.array(sequences)\n",
    "split_idx = int(0.9 * min_sequences)\n",
    "\n",
    "X_short_term_train = lstm_X[:split_idx]\n",
    "X_short_term_test = lstm_X[split_idx:]\n",
    "X_historical_train = sequences[:split_idx]\n",
    "X_historical_test = sequences[split_idx:]\n",
    "y_train = y[:split_idx]\n",
    "y_test = y[split_idx:]\n",
    "\n",
    "print(f\"X_short_term_train shape: {X_short_term_train.shape}\")\n",
    "print(f\"X_short_term_test shape: {X_short_term_test.shape}\")\n",
    "print(f\"X_historical_train shape: {X_historical_train.shape}\")\n",
    "print(f\"X_historical_test shape: {X_historical_test.shape}\")\n",
    "print(f\"y_train shape: {y_train.shape}\")\n",
    "print(f\"y_test shape: {y_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRID LSTM Model Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to create the GRID LSTM Model as we already imported the required modeules respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Create_Model():\n",
    "    input0 = Input(shape=(\"shape of the conv layer\"))\n",
    "    input2 = Input(shape=(\"shape of the lstm layer\"), name='input2')\n",
    "    lstm_outputs=[]\n",
    "    for i in range(\"range depends on the no of parellel lstm used in the model\"):\n",
    "        conv_layer = Conv1D(filters=\"mention no of filters needed\", kernel_size=1, activation='', padding='')(input0)\n",
    "        lstm_layer = LSTM(\"number of neurons needed\", return_sequences=True, dropout=.2, name='lstm{}'.format(i))(conv_layer)\n",
    "        lstm_layer1 = LSTM(\"number of neurins needed\", return_sequences=True, dropout=.2)(lstm_layer)\n",
    "        lstm_outputs.append(lstm_layer1)\n",
    "\n",
    "    concatenated_output = Concatenate()(lstm_outputs)\n",
    "    out = LSTM(\"number of neurins needed\", activation='', return_sequences=False, name='out')(concatenated_output)\n",
    "    dense = TimeDistributed(Dense(\"number of neurins needed\", activation=''))(input2)\n",
    "    lstm_out1 = LSTM(\"number of neurins needed\", activation='relu', return_sequences=False, name='lstm_out1')(dense)\n",
    "    x = Concatenate()([out, lstm_out1])\n",
    "    output1 = Dense(1, activation='linear', name='output1')(x)\n",
    "    model = Model(inputs=[input0, input2], outputs=output1)\n",
    "    model.compile(optimizer='', loss='', metrics=[''])\n",
    "    return model\n",
    "\n",
    "model = Create_Model()\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We created the model for the GRID LSTM, Now we have to train the model by using the Training Data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit([X_historical_train, X_short_term_train], y_train, validation_split=\"Mention the validation split percentage\" ,epochs=\"mention number of time you want to train the model\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upto to this is the model creation and the model training.\n",
    "\n",
    "Let's Predict the values using the model we trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_values = model.predict([X_historical_test, X_short_term_test])\n",
    "\n",
    "predictions_rescaled = scaler.inverse_transform(predicted_values.reshape(-1, 1)).flatten()\n",
    "y_test_rescaled = scaler.inverse_transform(y_test.reshape(-1, 1)).flatten()\n",
    "\n",
    "min_length = min(len(predictions_rescaled), len(y_test_rescaled))\n",
    "predictions_rescale = predictions_rescaled[:min_length]\n",
    "y_test_rescale = y_test_rescaled[:min_length]\n",
    "\n",
    "results = pd.DataFrame(data={'Predictions': predictions_rescale, 'Actuals': y_test_rescale})\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the Trained Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have to save the moedel so we can load the model without re-training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"path_to_save_the_maodel\") # Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = load_model(\"path_to_save_the_maodel\") # Loading the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Values of the Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now , we have to check the loss values of the Model we trained so we can get the  accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = math.sqrt(mean_squared_error(y_test_rescale, predictions_rescale))\n",
    "mae = mean_absolute_error(y_test_rescale, predictions_rescale)\n",
    "mape = mean_absolute_percentage_error(predictions_rescale, y_test_rescale)\n",
    "\n",
    "# Displaying the RMSE, MAE and MAPE values\n",
    "\n",
    "print('RMSE:', rmse)\n",
    "print('MAE:', mae)\n",
    "print('MAPE:', mape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.legend(['train', 'Validation'], loc='upper left')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
