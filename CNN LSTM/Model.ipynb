{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1Dimensional Convolution Neural Network Long Short Term Memory (1D-CNN LSTM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**what is 1D-CNN LSTM?**\n",
    "\n",
    "- Extracts and identifies key features from sequential data to enhance pattern recognition for improved model performance.\n",
    "\n",
    "- 1D-CNN Model is Used in the many Applications, Including Time Series Analysis, Signal Processing , Text Classifications etc..\n",
    "\n",
    "**Key Features:**\n",
    "- Spatial Feature Helps to identify key features and trends from Different station data that are important for making accurate predictions.\n",
    "\n",
    "- Captures the Temporal Features from the data and Analyzes the changes and patterns over time within the data.\n",
    "\n",
    "- Feature Fusion will Combine all features into one feature vector.\n",
    "\n",
    "- Letâ€™s see the Structure of the 1D-CNN Model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Below are the Required Modules for this Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required Models for the project\n",
    "\n",
    "import pandas as pd # type: ignore\n",
    "import numpy as  np # type: ignore \n",
    "import os # type: ignore \n",
    "import math # type: ignore\n",
    "import matplotlib.pyplot as plt # type: ignore\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler # type: ignore \n",
    "from tensorflow.keras.models import Model ,save_model ,load_model # type: ignore\n",
    "from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, LSTM, Dense, Flatten, concatenate # type: ignore\n",
    "from tensorflow.keras.losses import MeanSquaredError # type: ignore\n",
    "from tensorflow.keras.optimizers import Adam # type: ignore\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have to load our dataset required for the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_1 = 'PATH_TO_YOUR_DATASET.csv' # Path to your dataset\n",
    "station1 = pd.read_csv(file_path_1) # Load the dataset\n",
    "file_path_2 = 'PATH_TO_YOUR_DATASET.csv' # Path to your dataset\n",
    "station2 = pd.read_csv(file_path_2) # Load the dataset\n",
    "file_path_3 = 'PATH_TO_YOUR_DATASET.csv' # Path to your dataset\n",
    "station3 = pd.read_csv(file_path_3) # Load the dataset\n",
    "'''\n",
    "So on the number stations depends upon you we tried \n",
    "on the 4 stations and the results were good\n",
    "feel free to experiment the model with more stations'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to merge all the stations data to the 1 dataset.\n",
    "\n",
    "So, take the required columns from the dataset and combine the datasets to 1 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_station = station1.merge(station2, on='Time Steps', suffixes=('_1', '_2'))\n",
    "combined_station = combined_station.merge(station3, on='Time Steps', suffixes=('', '_3'))\n",
    "Flows = ['Time Steps', 'Flow_1', 'Flow_2', 'Flow_3']\n",
    "combined_station = combined_station[Flows]\n",
    "combined_station.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make sure There is no zero values in the dataset,we have to check is there any null values in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_values = combined_station.isnull().sum() # Check for null values\n",
    "print(\"Null values in each column:\\n\", null_values ) # Print the null values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, We have to get the required data values in between 0 and 1 to make the caculations simpler.\n",
    "\n",
    "So, we are using the Normalizatioin Method.\n",
    "\n",
    "We have to use 2 Scalers.\n",
    "\n",
    "- 1 scaler to fit the 4 stations combined dataset to get the spatial features from the 4 stations data.\n",
    "\n",
    "- 2 scaler to fit the point of intrest data set only Since we are going to predict the values in that stations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_scaler = MinMaxScaler()\n",
    "column1= ['Column Name']\n",
    "\n",
    "my_station_df = pd.DataFrame(columns=column1)\n",
    "my_station_df[column1] = combined_station[column1]\n",
    "\n",
    "my_station_df[column1] = target_scaler.fit_transform(my_station_df[column1])\n",
    "my_station_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column = ['Flow_1', 'Flow_2', 'Flow_3']\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "combined_station[column] = scaler.fit_transform(combined_station[column])\n",
    "combined_station.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to divide the data into the sequences according to the required number of TimeIntervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create sequences\n",
    "\n",
    "def create_sequences(column_data, time_steps):\n",
    "    X, Y = [], []\n",
    "    for i in range(len(column_data) - time_steps):\n",
    "        X.append(column_data[i:i + time_steps])\n",
    "        Y.append(column_data[i + time_steps])\n",
    "    return np.array(X), np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_data = combined_station[['Flow_1', 'Flow_2', 'Flow_3']].values\n",
    "time_steps = 'No_of_time_steps' # Number of time steps   \n",
    "input_dimension = 1 # Input dimension\n",
    "\n",
    "CNN_X, _ = create_sequences(column_data, time_steps)\n",
    "\n",
    "station1_data = column_data[:, 0].reshape(-1, 1)\n",
    "LSTM_X, y = create_sequences(station1_data, time_steps)\n",
    "\n",
    "# DISPLAYING THE VALUES OF THE VARIABLES\n",
    "\n",
    "print(f\"Values of cnn_X: {CNN_X}\")\n",
    "print(f\"Values of lstm_X: {LSTM_X}\")\n",
    "print(f\"Values of y: {y}\")\n",
    "\n",
    "print(f\"Shape of cnn_X: {CNN_X.shape}\")\n",
    "print(f\"Shape of lstm_X: {LSTM_X.shape}\")\n",
    "print(f\"Shape of y: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spliting the Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now , we have to split the data for training and testing the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into training top 90% and testing bottom 10%\n",
    "\n",
    "train_size = int(len(CNN_X) * 0.90)\n",
    "test_size = len(CNN_X) - train_size\n",
    "\n",
    "X_train_cnn, X_test_cnn = CNN_X[:train_size], CNN_X[train_size:]\n",
    "X_train_lstm, X_test_lstm = LSTM_X[:train_size], LSTM_X[train_size:]\n",
    "y_train, y_test = y[:train_size], y[train_size:]\n",
    "\n",
    "# Displaying the shapes of the training and testing data\n",
    "\n",
    "print(X_train_cnn.shape)\n",
    "print(X_train_lstm.shape)\n",
    "print(X_test_cnn.shape)\n",
    "print(X_test_lstm.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "print(\"Training CNN X Values : \",X_train_cnn)\n",
    "print(\"Training LSTM X Values : \",X_train_lstm)\n",
    "print(\"Testing CNN X Values : \",X_test_cnn)\n",
    "print(\"Testing LSTM X Values : \",X_test_lstm)\n",
    "print(\"Training y Values : \",y_train)\n",
    "print(\"Testing y Values : \",y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1D-CNN LSTM Model Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to create the 1D-CNN LSTM Model as we already imported the required modeules respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_input = Input(shape=(time_steps, 4)) # 4 is the number of features which is equal to the number of stations we used in the dataset\n",
    "cnn_layer = Conv1D(filters=\"number of filters Required\", kernel_size=2, activation='')(cnn_input) # Number of filters depends upon you\n",
    "cnn_layer = Flatten()(cnn_layer) # Flatten the layer\n",
    "\n",
    "lstm_input = Input(shape=(time_steps, 1)) # 1 is the number of features which is the point of interest we are going to predict the model \n",
    "lstm_layer = LSTM(\"number of neurons required\", activation='', return_sequences=False)(lstm_input) # Number of neurons depends upon you\n",
    "\n",
    "combined = concatenate([cnn_layer, lstm_layer]) # Concatenate the layers\n",
    "output = Dense(1)(combined) # Dense layer with 1 neuron\n",
    "\n",
    "model = Model(inputs=[cnn_input, lstm_input], outputs=output) # Create the model\n",
    "\n",
    "model.compile(optimizer='', loss='') # Compile the model\n",
    "\n",
    "model.summary() # Display the model summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We created the model for the 1D-CNN LSTM, Now we have to train the model by using the Training Data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit([X_train_cnn, X_train_lstm], y_train, epochs=\"number of times you want to train the model\", validation_split=\"Menton the validaion split here\") # Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upto to this is the model creation and the model training.\n",
    "\n",
    "Let's Predict the values using the model we trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_values = model.predict([X_test_cnn , X_test_lstm] ) # Predict the values\n",
    "\n",
    "predictions_rescaled = target_scaler.inverse_transform(predicted_values.reshape(-1, 1)).flatten() # Rescale the predictions\n",
    "y_test_rescaled = target_scaler.inverse_transform(y_test.reshape(-1, 1)).flatten() # Rescale the actual values\n",
    "\n",
    "results = pd.DataFrame(data={'Predictions': predictions_rescaled, 'Actuals': y_test_rescaled}) # Create a dataframe of the results\n",
    "print(results) # Display the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the Trained Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have to save the moedel so we can load the model without re-training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"path_to_save_the_maodel\") # Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = load_model(\"path_to_save_the_maodel\") # Loading the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Values of the Model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now , we have to check the loss values of the Model we trained so we can get the  accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = model.evaluate([X_test_cnn, X_test_lstm], y_test)\n",
    "print(f'Test Loss: {loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = math.sqrt(mean_squared_error(y_test_rescaled, predictions_rescaled))\n",
    "mae = mean_absolute_error(y_test_rescaled, predictions_rescaled)\n",
    "mape = mean_absolute_percentage_error(predictions_rescaled, y_test_rescaled)\n",
    "\n",
    "# Displaying the RMSE, MAE and MAPE values\n",
    "\n",
    "print('RMSE:', rmse)\n",
    "print('MAE:', mae)\n",
    "print('MAPE:', mape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.legend(['train', 'Validation'], loc='upper left')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
